{
 "cells": [
  {
   "cell_type": "code",
   "id": "2ba68eb0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-28T01:01:08.798078Z",
     "iopub.status.busy": "2024-08-28T01:01:08.797582Z",
     "iopub.status.idle": "2024-08-28T01:01:26.987277Z",
     "shell.execute_reply": "2024-08-28T01:01:26.986037Z"
    },
    "id": "-3f9T0JOkoVb",
    "papermill": {
     "duration": 18.19929,
     "end_time": "2024-08-28T01:01:26.990136",
     "exception": false,
     "start_time": "2024-08-28T01:01:08.790846",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "import keras # type: ignore\n",
    "import tensorflow as tf # type: ignore\n",
    "import matplotlib.pyplot as plt # type: ignore\n",
    "from keras import layers, optimizers, ops # type: ignore\n",
    "from keras_tuner import (HyperModel,\n",
    "                         Hyperband,\n",
    "                         RandomSearch,\n",
    "                         Objective,\n",
    "                         BayesianOptimization)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "284efe33",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-28T01:01:27.001996Z",
     "iopub.status.busy": "2024-08-28T01:01:27.001303Z",
     "iopub.status.idle": "2024-08-28T01:01:27.011844Z",
     "shell.execute_reply": "2024-08-28T01:01:27.010557Z"
    },
    "id": "gjfw-qt6IbqQ",
    "papermill": {
     "duration": 0.019107,
     "end_time": "2024-08-28T01:01:27.014409",
     "exception": false,
     "start_time": "2024-08-28T01:01:26.995302",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "class TransformerEncoder(layers.Layer):\n",
    "    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
    "        super(TransformerEncoder, self).__init__(**kwargs)\n",
    "        self.embed_dim = embed_dim\n",
    "        self.dense_dim = dense_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.attention = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.dense_proj = keras.Sequential(\n",
    "            [layers.Dense(dense_dim, activation=\"relu\"),\n",
    "             layers.Dense(embed_dim),]\n",
    "        )\n",
    "        self.layernorm_1 = layers.LayerNormalization()\n",
    "        self.layernorm_2 = layers.LayerNormalization()\n",
    "\n",
    "    def call(self, inputs, mask=None):\n",
    "        if mask is not None:\n",
    "            mask = mask[:, tf.newaxis, :]\n",
    "        attention_output = self.attention(\n",
    "            inputs, inputs, attention_mask=mask)\n",
    "        proj_input = self.layernorm_1(inputs + attention_output)\n",
    "        proj_output = self.dense_proj(proj_input)\n",
    "        return self.layernorm_2(proj_input + proj_output)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            \"embed_dim\": self.embed_dim,\n",
    "            \"num_heads\": self.num_heads,\n",
    "            \"dense_dim\": self.dense_dim,\n",
    "        })\n",
    "        return config"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "7db048d1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-28T01:01:27.025928Z",
     "iopub.status.busy": "2024-08-28T01:01:27.025526Z",
     "iopub.status.idle": "2024-08-28T01:01:27.036170Z",
     "shell.execute_reply": "2024-08-28T01:01:27.034855Z"
    },
    "id": "DPCtXeAFIIeA",
    "papermill": {
     "duration": 0.019381,
     "end_time": "2024-08-28T01:01:27.038660",
     "exception": false,
     "start_time": "2024-08-28T01:01:27.019279",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "class PositionalEmbedding(layers.Layer):\n",
    "    def __init__(self, sequence_length, input_dim, output_dim, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.token_embeddings = layers.Embedding(\n",
    "            input_dim=input_dim,\n",
    "            output_dim=output_dim)\n",
    "\n",
    "        self.position_embeddings = layers.Embedding(\n",
    "            input_dim=sequence_length,\n",
    "            output_dim=output_dim)\n",
    "\n",
    "        self.sequence_length = sequence_length\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.supports_masking = True\n",
    "\n",
    "\n",
    "    def call(self, inputs):\n",
    "        length = ops.shape(inputs)[-1]\n",
    "        positions = ops.arange(start=0, stop=length, step=1)\n",
    "        embedded_tokens = self.token_embeddings(inputs)\n",
    "        embedded_positions = self.position_embeddings(positions)\n",
    "        return embedded_tokens + embedded_positions\n",
    "\n",
    "    def compute_mask(self, inputs, mask=None):\n",
    "        return keras.ops.not_equal(inputs, 0)  \n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.position_embeddings.build(input_shape)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            \"output_dim\": self.output_dim,\n",
    "            \"sequence_length\": self.sequence_length,\n",
    "            \"input_dim\": self.input_dim,\n",
    "        })\n",
    "        return config"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "166114f7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-28T01:01:27.049826Z",
     "iopub.status.busy": "2024-08-28T01:01:27.049406Z",
     "iopub.status.idle": "2024-08-28T01:01:27.062181Z",
     "shell.execute_reply": "2024-08-28T01:01:27.061011Z"
    },
    "executionInfo": {
     "elapsed": 1860573,
     "status": "ok",
     "timestamp": 1724129616963,
     "user": {
      "displayName": "Ulilé indique",
      "userId": "14283319943525905449"
     },
     "user_tz": 180
    },
    "id": "OxlFWmWUz_9R",
    "outputId": "6843805a-5da1-475b-b408-f945f0b10a0f",
    "papermill": {
     "duration": 0.021414,
     "end_time": "2024-08-28T01:01:27.064766",
     "exception": false,
     "start_time": "2024-08-28T01:01:27.043352",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "class HyperbandSearch():\n",
    "    def __init__(self, max_length, vocab_size, train_ds, vals_ds, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.train_ds, self.vals_ds = train_ds, vals_ds\n",
    "        self.max_length = max_length\n",
    "        self.vocab_size = vocab_size\n",
    "\n",
    "    def build_model_hyperband(self, hp):\n",
    "        embed_dim = hp.Int('embed_dim', min_value=128, max_value=2048, step=128)\n",
    "        num_heads = hp.Int('num_heads', min_value=2, max_value=8, step=1)\n",
    "        dense_dim = hp.Int('dense_dim', min_value=32, max_value=512, step=32)\n",
    "        dropout_rate = hp.Float('dropout_rate', min_value=0.3, max_value=0.9, step=0.1)\n",
    "        learning_rate = hp.Float('learning_rate', min_value=1e-5, max_value=1e-2, sampling='log')\n",
    "\n",
    "        inputs = keras.Input(shape=(None,), dtype=\"int64\")\n",
    "        x = PositionalEmbedding(\n",
    "            self.max_length,\n",
    "            self.vocab_size, embed_dim)(inputs)\n",
    "        x = TransformerEncoder(embed_dim, dense_dim, num_heads)(x)\n",
    "        x = layers.GlobalMaxPooling1D()(x)\n",
    "        x = layers.Dropout(dropout_rate)(x)\n",
    "\n",
    "        outputs = layers.Dense(1)(x)\n",
    "\n",
    "        model = keras.Model(inputs, outputs)\n",
    "        model.compile(\n",
    "            optimizer=optimizers.Adam(learning_rate=learning_rate),\n",
    "            loss=\"mae\")\n",
    "\n",
    "        return model\n",
    "\n",
    "    def find_w_hyperband(self):\n",
    "        tuner = Hyperband(\n",
    "            self.build_model_hyperband,\n",
    "            objective='val_loss',\n",
    "            max_epochs=30,\n",
    "            factor=3,\n",
    "            directory='my_dir',\n",
    "            project_name='transformer_tuning'\n",
    "        )\n",
    "\n",
    "        tuner.search(\n",
    "            self.train_ds,\n",
    "            epochs=60,\n",
    "            validation_data=self.vals_ds)\n",
    "        hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "        return hps.values"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "6a11fd98",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-28T01:01:27.075920Z",
     "iopub.status.busy": "2024-08-28T01:01:27.075514Z",
     "iopub.status.idle": "2024-08-28T01:01:27.087560Z",
     "shell.execute_reply": "2024-08-28T01:01:27.086116Z"
    },
    "executionInfo": {
     "elapsed": 2087970,
     "status": "ok",
     "timestamp": 1724134660016,
     "user": {
      "displayName": "Ulilé indique",
      "userId": "14283319943525905449"
     },
     "user_tz": 180
    },
    "id": "9qbn8ItYCJ0r",
    "outputId": "4986027f-2f45-4a67-b23f-4dc01d06337b",
    "papermill": {
     "duration": 0.020944,
     "end_time": "2024-08-28T01:01:27.090444",
     "exception": false,
     "start_time": "2024-08-28T01:01:27.069500",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "class RandomRearch():\n",
    "    def __init__(self, max_length, vocab_size, train_ds, vals_ds, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.train_ds, self.vals_ds = train_ds, vals_ds\n",
    "        self.max_length = max_length\n",
    "        self.vocab_size = vocab_size\n",
    "\n",
    "    def build_model_random(self, hp):\n",
    "        embed_dim = hp.Int('embed_dim', min_value=128, max_value=2048, step=128)\n",
    "        num_heads = hp.Int('num_heads', min_value=2, max_value=8, step=1)\n",
    "        dense_dim = hp.Int('dense_dim', min_value=32, max_value=512, step=32)\n",
    "        dropout_rate = hp.Float('dropout_rate', min_value=0.3, max_value=0.7, step=0.1)\n",
    "        learning_rate = hp.Float('learning_rate', min_value=1e-5, max_value=1e-2, sampling='log')\n",
    "\n",
    "        inputs = keras.Input(shape=(None,), dtype=\"int64\")\n",
    "        #x = layers.Embedding(self.vocab_size, embed_dim)(inputs)\n",
    "        x = PositionalEmbedding(\n",
    "            self.max_length,\n",
    "            self.vocab_size, embed_dim)(inputs)\n",
    "        x = TransformerEncoder(embed_dim, dense_dim, num_heads)(x)\n",
    "        x = layers.GlobalMaxPooling1D()(x)\n",
    "        x = layers.Dropout(dropout_rate)(x)\n",
    "\n",
    "        outputs = layers.Dense(1)(x)\n",
    "\n",
    "        model = keras.Model(inputs, outputs)\n",
    "        model.compile(\n",
    "            optimizer=optimizers.Adam(learning_rate=learning_rate),\n",
    "            loss=\"mae\")\n",
    "\n",
    "        return model\n",
    "\n",
    "    def find_w_random(self):\n",
    "        tuner = RandomSearch(\n",
    "            self.build_model_random,\n",
    "            objective='val_loss',\n",
    "            max_trials=30,  # Número de combinações a serem testadas\n",
    "            executions_per_trial=1,\n",
    "            directory='my_dir_rnd',\n",
    "            project_name='transformer_randomsearch'\n",
    "        )\n",
    "\n",
    "        tuner.search(\n",
    "            self.train_ds,\n",
    "            epochs=60,\n",
    "            validation_data=self.vals_ds)\n",
    "\n",
    "        best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "        return best_hps.values"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "7ad4d8a3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-28T01:01:27.101534Z",
     "iopub.status.busy": "2024-08-28T01:01:27.101162Z",
     "iopub.status.idle": "2024-08-28T01:01:27.114760Z",
     "shell.execute_reply": "2024-08-28T01:01:27.113695Z"
    },
    "executionInfo": {
     "elapsed": 3817632,
     "status": "ok",
     "timestamp": 1724138477647,
     "user": {
      "displayName": "Ulilé indique",
      "userId": "14283319943525905449"
     },
     "user_tz": 180
    },
    "id": "c4WFVDZJDqAD",
    "outputId": "ce1bd008-dd0c-4960-faaa-7a62de85b5bc",
    "papermill": {
     "duration": 0.022278,
     "end_time": "2024-08-28T01:01:27.117451",
     "exception": false,
     "start_time": "2024-08-28T01:01:27.095173",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "class BayesianRearch(HyperModel):\n",
    "    def __init__(self, max_length, vocab_size, train_ds, vals_ds, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.train_ds, self.vals_ds = train_ds, vals_ds\n",
    "        self.max_length = max_length\n",
    "        self.vocab_size = vocab_size\n",
    "\n",
    "    def build(self, hp):\n",
    "        embed_dim = hp.Int('embed_dim', min_value=256, max_value=2048, step=256)\n",
    "        num_heads = hp.Int('num_heads', min_value=2, max_value=8, step=1)\n",
    "        dense_dim = hp.Int('dense_dim', min_value=64, max_value=512, step=64)\n",
    "        dropout_rate = hp.Float('dropout_rate', min_value=0.3, max_value=0.7, step=0.1)\n",
    "        optimizer = hp.Choice(name=\"optimizer\", values=[\"adamw\", \"adam\", \"sgd\"])\n",
    "        learning_rate = hp.Float('learning_rate', min_value=1e-5, max_value=1e-2, sampling='log')\n",
    "\n",
    "        inputs = keras.Input(shape=(None,), dtype=\"int64\")\n",
    "        #x = layers.Embedding(vocab_size, embed_dim)(inputs)\n",
    "        x = PositionalEmbedding(\n",
    "            self.max_length,\n",
    "            self.vocab_size, embed_dim)(inputs)\n",
    "        x = TransformerEncoder(embed_dim, dense_dim, num_heads)(x)\n",
    "        x = layers.GlobalMaxPooling1D()(x)\n",
    "        x = layers.Dropout(dropout_rate)(x)\n",
    "\n",
    "        outputs = layers.Dense(1)(x)\n",
    "\n",
    "        model = keras.Model(inputs, outputs)\n",
    "        model.compile(\n",
    "            optimizer=optimizer,\n",
    "            loss=\"mae\")\n",
    "\n",
    "        return model\n",
    "\n",
    "    def call_bayesian(self):\n",
    "        objective = Objective(\n",
    "            name=\"val_loss\",\n",
    "            direction=\"min\"\n",
    "        )\n",
    "        tuner = BayesianOptimization(\n",
    "            self.build,\n",
    "            objective=objective,\n",
    "            max_trials=100,  # Número de combinações a serem testadas\n",
    "            executions_per_trial=2,\n",
    "            directory='price_prediction',\n",
    "            project_name='transformer_bayesian',\n",
    "            overwrite=True\n",
    "        )\n",
    "        display(tuner.search_space_summary())\n",
    "        callbacks = [\n",
    "            keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=5),\n",
    "        ]\n",
    "        tuner.search(\n",
    "            self.train_ds,\n",
    "            epochs=100,\n",
    "            callbacks=callbacks,\n",
    "            verbose=2,\n",
    "            validation_data=self.vals_ds\n",
    "        )\n",
    "\n",
    "        return tuner"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": [
    {
     "file_id": "1eqvkRwEPuTqTTaRYEE-EI-9JbOJUqvhn",
     "timestamp": 1724162301371
    }
   ]
  },
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 30746,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 54.715332,
   "end_time": "2024-08-28T01:01:28.749266",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-08-28T01:00:34.033934",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
